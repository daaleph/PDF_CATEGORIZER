=================================================
PROJECT CONTEXT FOR AI ANALYSIS
=================================================
This document contains the project structure and source code for review.
Project root: /home/dc/Documents/TOOLS/PDF-CATEGORIZER
Generated on: Mon Jan 26 12:06:08 PM -05 2026

-------------------------------------------------
PROJECT FILE AND FOLDER STRUCTURE
-------------------------------------------------

.
├── =3.1
├── ai_classifier.py
├── book_classifications.jsonl
├── BOOKS
│   ├── AllofStatistics_LarryWasserman_2004.pdf
│   ├── ConvexOptimizationTheory_DimitriPBertsekas_2009.pdf
│   ├── DynamicalSystems_Oliver-Knill_2005_Pdf.pdf
│   ├── IntroductiontoCalculusandAnalysisVolII_RichardCourant-FritzJohn.pdf
│   ├── IntroductionToCalculusAndAnalysisVolI_RichardCourant-FritzJohn.pdf
│   ├── IntroductionToLinearOptimization_DimitriBertsimas-JohnTsitsiklis_1997.pdf
│   ├── IntroductionToStochasticProcesses_GregoryFLawler.pdf
│   ├── LectureNotesOnProbability-StatisticsAndLinearAlgebra_C-H-Taubes_2010_Pdf.pdf
│   ├── LinearAlgebraAndVectorAnalysisMath22b_Oliver-Knill_2019_Pdf.pdf
│   ├── LinearAlgebraWithProbability_Oliver-Knill_2011_Pdf.pdf
│   ├── Math1a-IntroductionToFunctionsAndCalculus_Oliver-Knill_2014.pdf
│   ├── MultivariableCalculus-MathS21a_Oliver-Knill_2019_Pdf.pdf
│   ├── NumericalAnalysis_JDouglasFaires_2016.pdf
│   ├── ProbabilityTheoryAndStochasticProcessesWithApplications_Oliver-Knill_2009_Pdf.pdf
│   ├── TeachingMathWithAHistoricalPerspective_Oliver-Knill_2014_Pdf.pdf
│   ├── TheDesignOfApproximationAlgorithms_DavidP.Williamson-DavidB.Shmoys_2012.pdf
│   └── Unit1-PythagoreanTheorem_Oliver-Knill_2018_Pdf.pdf
├── contextualizer.sh
├── diagrams
│   ├── diagram.puml
│   └── output
│       └── diagram.png
├── .env
├── extract_chapter.py
├── get_gemini_response.py
├── .gitignore
├── graphs
│   ├── chart_1_overall_composition.png
│   ├── chart_2_complexity_by_category.png
│   ├── chart_3_diagnostic_heatmap_wrapped_labels.png
│   ├── chart_4_outline_scatter.png
│   ├── chart_5_complexity_anatomy_subplots.png
│   ├── chart_6_metadata_by_class.png
│   ├── comparative_view.py
│   ├── compiling.md
│   ├── complexity_anatomy.py
│   ├── load_results.py
│   ├── metadata_analysis_by_class.py
│   ├── metadata_quality.py
│   ├── overall_corpus_composition.py
│   ├── pipeline_performance.py
│   └── rationale.py
├── layout_analyzer.py
├── logs
│   ├── segmentation.jsonl
│   └── skipped_books.txt
├── metadata_checker.py
├── pipe.py
├── prompt_generator.py
├── README.md
├── requirements.txt
├── segmentation_pipe.py
├── segmented_output
│   └── BOOKS
└── StructuralComplexityClassificationSystem.md

8 directories, 52 files


-------------------------------------------------
PROJECT SOURCE CODE FILES
-------------------------------------------------

=========================================
FILE: get_gemini_response.py
=========================================
```python
#!/usr/bin/env python3
"""
Communicates with the Google Gemini API using the official Python SDK.
This version includes detailed logging, handles rate limit retries, and uses 
the correct client initialization and API call patterns.
"""
import os
import time
import random
import logging
from dotenv import load_dotenv
import google.genai as genai

# --- Logging Configuration ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# --- API Key Configuration ---
# The list of keys provided in the prompt, mapped in sequential order.
API_KEYS = [
    "AIzaSyBzEFcmZyCPmCrQi_ogXK0TpdlBsmxWePs", # GEMINI_API_KEY_D
    "AIzaSyD9myFJkZ7fr-2az5vcwbBRK_jro_OX8RY", # GEMINI_API_KEY_M
    "AIzaSyBuESGsJaE5tw2NsbTuR8KbtmUeEiPfEdw", # GEMINI_API_KEY_N_1
    "AIzaSyB1dbpi8TKr0Es-Z5pIKFovnlNoUthNGmA", # GEMINI_API_KEY_N_P1
    "AIzaSyC9yPtCmGT66GdeEv54PtgFtGlYm6xkI0w",
    "AIzaSyCCJBUkLqTLqZUMUiKNjBz8gXv2KuVI-zA",
    "AIzaSyAnMobQefL_aW4zjP5g789ZWtSlaHVchyo", # GEMINI_API_KEY_N_2
    "AIzaSyCkGtw5JRjwh2I-7bhPFph851szetsrOmE", # GEMINI_API_KEY_N_P2
    "AIzaSyCIwbZa1gUifhDW4ZbNq2L5d1jc229sq9I", # GEMINI_API_KEY_N_3
    "AIzaSyDJ1RaWuv2ssFA-9ZY6OYeLIgwEawLOl8s"  # GEMINI_API_KEY_N_P3
]

# Load environment variables
logging.info("Loading environment variables from .env file...")
load_dotenv()

# --- SDK Configuration ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not API_KEYS:
    logging.critical("Error: API_KEYS list is empty. Exiting.")
    exit(1)
else:
    logging.info(f"Loaded {len(API_KEYS)} API keys for rotation.")

# --- Configuration for Intelligent Retries ---
MAX_CYCLES = 10 
INITIAL_BACKOFF_SECONDS = 60
MAX_BACKOFF_SECONDS = 600

def get_gemini_response(prompt: str, model: str = 'gemini-2.5-flash') -> str:
    """
    Sends a prompt to the Google Gemini API.
    Implements round-robin rotation of API keys on rate limit errors.
    """
    logging.info(f"Preparing to send prompt to Gemini model: '{model}'")
    logging.info(f"Prompt (first 70 chars): \"{prompt[:70].replace('\n', ' ')}...\"")
    logging.debug(f"Full prompt being sent:\n---\n{prompt}\n---")

    cycle_count = 0
    total_keys = len(API_KEYS)

    # Outer loop: Handles waiting (backoff) after exhausting all keys
    while cycle_count < MAX_CYCLES:
        
        # Inner loop: Iterates through available keys
        for key_index, api_key in enumerate(API_KEYS):
            
            try:
                logging.info(f"Cycle {cycle_count + 1}/{MAX_CYCLES} | Key {key_index + 1}/{total_keys}: Calling Gemini API...")
                
                # Initialize a fresh client for this specific key
                client = genai.Client(api_key=api_key)
                
                # API Call
                response = client.models.generate_content(
                    model=model,
                    contents=prompt
                )
                
                logging.info(f"Cycle {cycle_count + 1}/{MAX_CYCLES} | Key {key_index + 1}/{total_keys}: API call successful.")
                
                if response.text:
                    logging.info("Response contains text. Returning content.")
                    return response.text.strip()
                else:
                    logging.warning("Response received, but it contains no text. It may have been blocked by safety filters.")
                    raise RuntimeError("API responded with no text content. Check safety settings or prompt feedback.")

            except Exception as e:
                error_message_upper = str(e).upper()
                
                # Check for Rate Limit (429) or Resource Exhausted
                if "429" in error_message_upper or "RESOURCE_EXHAUSTED" in error_message_upper:
                    logging.warning(
                        f"Cycle {cycle_count + 1} | Key {key_index + 1}: Rate limit hit. "
                        f"Switching to next key immediately."
                    )
                    # Continue to the next iteration of the for-loop (next key)
                    continue
                else:
                    # Non-recoverable error (e.g., 400 Bad Request, 401 Invalid Key)
                    logging.error(f"A non-recoverable error occurred with Key {key_index + 1}.", exc_info=True)
                    raise RuntimeError(f"Gemini API call failed with a non-recoverable error: {e}") from e

        # --- If we reach this point, the 'for' loop finished without returning ---
        # This means ALL keys in the list have hit their rate limit in this cycle.
        
        if cycle_count < MAX_CYCLES - 1:
            backoff_time = min(INITIAL_BACKOFF_SECONDS * (2 ** cycle_count), MAX_BACKOFF_SECONDS)
            jitter = random.uniform(0, 5)
            wait_time = backoff_time + jitter
            
            logging.critical(
                f"All {total_keys} API keys exhausted in Cycle {cycle_count + 1}. "
                f"Waiting {wait_time:.2f} seconds before restarting from Key 1."
            )
            time.sleep(wait_time)
        else:
            logging.critical(f"All API keys failed after {MAX_CYCLES} full cycles.")
            raise RuntimeError(f"Exhausted all retries. All {total_keys} keys persistently rate-limited.")

        cycle_count += 1

    raise RuntimeError(f"Failed to get response for prompt after {MAX_CYCLES} cycles.")
```


=========================================
FILE: README.md
=========================================
```markdown
# PDF_CATEGORIZER & SEGMENTATION System

## 1. Introduction

The **PDF_CATEGORIZER** system is a sophisticated two-phase pipeline designed for in-depth analysis and processing of large PDF collections.

1.  **Phase 1: Classification**: The system first analyzes each PDF to determine its **structural complexity**. It uses a combination of metadata analysis and layout inspection to classify books into categories, such as a "Simple Linear Monograph" (Level 1) or a "Standard Hierarchical Textbook" (Level 2). This crucial step identifies which books have reliable, machine-readable data.
2.  **Phase 2: Segmentation**: For books identified as having high-quality metadata, the system uses the Gemini AI to intelligently generate precise command-line instructions. It then executes these commands to automatically split the PDF into its constituent parts, such as the table of contents, individual chapters, and appendices.

This powerful workflow allows you to deconstruct an entire library of books into well-named, chapter-level files, making the content ready for targeted analysis, Retrieval-Augmented Generation (RAG), or ingestion into other AI models with limited context windows.

## 2. Prerequisites

This system relies on Python and several external command-line tools. Ensure you have the following installed:

*   **Python 3**: Download from [python.org](https://python.org/).
*   **Google Gemini API Key**: Obtain one from [Google AI for Developers](https://ai.google.dev/).
*   **pdftk**: A powerful command-line tool for manipulating PDFs. It must be installed and accessible from your system's PATH.
*   **Ghostscript**: Required for handling certain types of PDF processing and password removal. It must be installed and in your system's PATH.
*   **qpdf**: A command-line tool for PDF transformation, used here as a fallback for password removal. It must be installed and in your system's PATH.

## 3. Setup and Configuration

1.  **Download the Project**: Unzip or clone the `PDF_CATEGORIZER` project to your local machine.
2.  **Open a Terminal**: Navigate to the root directory of the `PDF_CATEGORIZER` project.
3.  **Install Dependencies**: It is highly recommended to use a Python virtual environment. Install the required packages using the following command:
    ```bash
    pip install google-generativeai python-dotenv pypdf PyMuPDF pandas
    ```
4.  **Create Environment File**: Create a new file named `.env` in the project's root directory.
5.  **Set API Key**: Open the `.env` file and add your Gemini API key:
    ```
    GEMINI_API_KEY=YOUR_API_KEY
    ```

## 4. How to Use the System: A Two-Step Process

**Step 1: Place Your PDFs**

Place the PDF books you wish to process into the `BOOKS/` directory. The system will recursively scan any subdirectories as well.

**Step 2: Run the Classification Pipeline**

The first step is to classify every book in your collection. Open your terminal in the project root and run:

```bash
python pipe.py
```

This script will:
*   Iterate through every PDF file it finds.
*   Use `metadata_checker.py` to check for a valid table of contents (bookmarks).
*   If no metadata is found, it will fall back to `layout_analyzer.py` to inspect the visual structure.
*   Send the collected evidence to the Gemini API, which assigns a structural complexity level to the book.
*   Save all results to a file named **`book_classifications.jsonl`**. This file is a detailed log of your collection and the input for the next phase.

You can also explore the `graphs/` folder, which contains scripts to visualize the classification results and gain insights into your corpus.

**Step 3: Run the Segmentation Pipeline**

After classification is complete, you can segment the eligible books. In the same terminal, run:

```bash
python segmentation_pipe.py
```

This script performs the following critical actions:
1.  It reads the `book_classifications.jsonl` file.
2.  It **filters for "safe" books**—those that were classified using `metadata_check`, as this confirms they have a reliable table of contents suitable for segmentation.
3.  For each safe book, it sends the bookmark data to the Gemini AI, instructing it to act as an expert and return a list of `pdftk` commands to extract each chapter.
4.  It receives the JSON response and executes each command, effectively splitting the PDF.

## 5. Understanding the Outputs

After running the pipeline, your project directory will contain the following:

*   **`segmented_output/`**: This directory contains the final result. It mirrors the structure of your original `BOOKS/` folder, but inside each book's subfolder, you will find the cleanly separated PDF files (e.g., `00_Title_Page.pdf`, `Chapter_01_Introduction.pdf`).
*   **`book_classifications.jsonl`**: The JSONL file containing the detailed classification results for every book. You can learn more about the classification levels by reading `StructuralComplexityClassificationSystem.md`.
*   **`logs/`**:
    *   `segmentation.jsonl`: A log detailing the success or failure of the segmentation process for each book.
    *   `skipped_books.txt`: A list of books that were not segmented, typically because they lacked the necessary metadata (i.e., they were classified as Level 5).
```


=========================================
FILE: graphs/compiling.md
=========================================
```markdown
# How to compile these puml files?

```bash
java -jar plantuml.jar -tpng -o output *.puml
```
```


=========================================
FILE: graphs/complexity_anatomy.py
=========================================
```python
# graphs/complexity_anatomy.py
import pandas as pd
import plotly.graph_objects as go
import plotly.subplots as sp
from sklearn.preprocessing import MinMaxScaler
from load_results import load_classification_data
import math

def create_complexity_subplot_radar_chart(df: pd.DataFrame):
    """
    Generates a figure with multiple radar chart subplots to visualize the 
    defining characteristics of each structural complexity level, avoiding overlap.
    """
    if df.empty:
        print("DataFrame is empty. No data to plot.")
        return

    # --- 1. Data Preparation (Same as before) ---
    metrics = [
        'has_pypdf_outline',
        'outline_depth',
        'outline_length',
        'distinct_font_sizes',
        'page_number_transition'
    ]
    
    df_radar = df.copy()
    df_radar[metrics[3]] = df_radar[metrics[3]].fillna(0)
    df_radar[metrics[4]] = df_radar[metrics[4]].fillna(False).astype(int)
    df_radar[metrics[0]] = df_radar[metrics[0]].astype(int)

    class_profile = df_radar.groupby('classification')[metrics].mean()

    # --- 2. Data Scaling (Crucial for comparison across different units) ---
    scaler = MinMaxScaler()
    scaled_profiles = pd.DataFrame(scaler.fit_transform(class_profile), 
                                   index=class_profile.index, 
                                   columns=class_profile.columns)
    
    # --- 3. Subplot Grid Calculation (NEW) ---
    # Dynamically determine the grid layout to accommodate all levels.
    levels = sorted(df['classification'].unique())
    num_levels = len(levels)
    cols = 3  # Let's aim for a max of 3 columns
    rows = math.ceil(num_levels / cols)
    
    # Create a list of specs for the subplots, specifying 'polar' type for each.
    specs = [[{'type': 'polar'}] * cols for _ in range(rows)]
    
    # Create subplot titles from the classification levels
    subplot_titles = [f"<b>{level}</b>" for level in levels]

    fig = sp.make_subplots(
        rows=rows, 
        cols=cols, 
        specs=specs, 
        subplot_titles=subplot_titles
    )

    # --- 4. Chart Creation with Subplots (ENHANCED) ---
    metric_labels = {
        'has_pypdf_outline': 'Has Bookmarks',
        'outline_depth': 'Hierarchy Depth',
        'outline_length': 'ToC Length',
        'distinct_font_sizes': 'Typographic Variety',
        'page_number_transition': 'Uses Roman Numerals'
    }
    radar_labels = [metric_labels[m] for m in metrics]

    # Loop through each level and add its trace to the correct subplot
    for i, level in enumerate(levels):
        # Plotly subplot indexing is 1-based
        row = i // cols + 1
        col = i % cols + 1

        fig.add_trace(go.Scatterpolar(
            r=scaled_profiles.loc[level].values,
            theta=radar_labels,
            fill='toself',
            name=level, # Although legend is hidden, name is good for hover data
            hovertemplate=f"<b>Characteristic:</b> %{{theta}}<br><b>Scaled Value:</b> %{{r:.2f}}<extra></extra>"
        ), row=row, col=col)

    # --- 5. Layout Update for Subplots (ENHANCED) ---
    # This part is more complex as we need to update the layout for the entire figure
    # and potentially each subplot's axes if needed.
    fig.update_layout(
        title={
            'text': "Anatomy of Structural Complexity Levels (Comparative View)",
            'y':0.97,
            'x':0.5,
            'xanchor': 'center',
            'yanchor': 'top',
            'font': {'size': 24, 'family': 'Arial, bold'}
        },
        showlegend=False,  # Legend is redundant as each subplot is titled
        font_family="Arial",
        height=rows * 450, # Dynamically adjust height based on number of rows
        width=1400,
        margin=dict(l=40, r=40, t=80, b=40)
    )

    # Standardize the radial axis for all subplots to ensure they are comparable
    polar_update = dict(
        radialaxis=dict(
            visible=True,
            range=[0, 1],
            showticklabels=False, # Hiding labels cleans up the look
            ticks=''
        )
    )
    fig.update_polars(polar_update)
    
    # Set the font size for the subplot titles
    for annotation in fig['layout']['annotations']:
        annotation['font'] = {'size': 16, 'family': 'Arial, bold'}


    fig.write_image("chart_5_complexity_anatomy_subplots.png", width=1400, height=rows * 450, scale=2)
    print("\nGenerated enhanced chart: chart_5_complexity_anatomy_subplots.png")
    fig.show()


if __name__ == '__main__':
    try:
        df_results = load_classification_data()
        # Ensure a logical order for plotting
        class_order = sorted(df_results['classification'].unique())
        df_results['classification'] = pd.Categorical(df_results['classification'], categories=class_order, ordered=True)
        
        create_complexity_subplot_radar_chart(df_results)
        
    except FileNotFoundError:
        print("Error: 'book_classifications.jsonl' not found. Run the main pipe.py script first.")
    except Exception as e:
        print(f"An error occurred: {e}")
```


=========================================
FILE: graphs/pipeline_performance.py
=========================================
```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# Add the textwrap import
import textwrap
from load_results import load_classification_data

# Load the data
df = load_classification_data()

# --- Chart 3: Analysis Method vs. Final Complexity ---
# Create the crosstab
crosstab = pd.crosstab(df['analysis_type'], df['classification'])

# Create the heatmap
fig, ax = plt.subplots(figsize=(10, 6)) # Increased height slightly for wrapped labels
sns.heatmap(crosstab, annot=True, fmt='d', cmap='YlGnBu', linewidths=.5, ax=ax)

# Add labels and title
ax.set_title('Analysis Method vs. Final Classification', fontsize=16, pad=20)
ax.set_ylabel('Analysis Method Used', fontsize=12)
ax.set_xlabel('Assigned Complexity Level', fontsize=12)

# --- NEW CODE FOR WRAPPING LABELS ---
# Get the current labels
labels = [label.get_text() for label in ax.get_xticklabels()]
# Wrap the labels to a specific width (e.g., 15 characters)
wrapped_labels = [textwrap.fill(label, 15) for label in labels]
# Set the new wrapped labels
ax.set_xticklabels(wrapped_labels)
# --- END OF NEW CODE ---

# Use tight_layout() to adjust the plot and prevent labels from being cut off
plt.tight_layout()
plt.savefig("chart_3_diagnostic_heatmap_wrapped_labels.png")
plt.show()
```


=========================================
FILE: graphs/overall_corpus_composition.py
=========================================
```python
import matplotlib.pyplot as plt
import seaborn as sns
from load_results import load_classification_data

def add_rationale_box(ax):
    """Adds an interpretation box to the chart."""
    rationale_text = (
        "How to Interpret This Chart:\n\n"
        "This chart provides a high-level overview of the corpus.\n"
        "It answers: What is the most common structural type?\n\n"
        "- A large bar for Level 2 indicates a collection of modern,\n"
        "  well-structured books with good metadata.\n"
        "- A large bar for Level 5 signifies that many books lack\n"
        "  metadata, making the layout analysis pipeline essential."
    )
    
    props = dict(boxstyle='round,pad=0.5', facecolor='aliceblue', alpha=0.9)
    
    # Place the text box in a suitable position.
    # The transform=ax.transAxes means coordinates are relative to the axes (0,0 is bottom-left, 1,1 is top-right).
    ax.text(0.95, 0.05, rationale_text, transform=ax.transAxes, fontsize=9,
            verticalalignment='bottom', horizontalalignment='right', bbox=props)


# Load the data
df = load_classification_data()

# --- Chart 1: Overall Corpus Composition ---
plt.style.use('seaborn-v0_8-whitegrid')
fig, ax = plt.subplots(figsize=(12, 7)) # Increased size slightly for the box

# Count the occurrences of each classification
classification_counts = df['classification'].value_counts().sort_values(ascending=True)

# Create the horizontal bar plot
bars = ax.barh(classification_counts.index, classification_counts.values, color=sns.color_palette("viridis", len(classification_counts)))

# Add labels and title
ax.set_title('Distribution of Structural Complexity Across All Books', fontsize=16, pad=20)
ax.set_xlabel('Number of Books', fontsize=12)
ax.set_ylabel('Complexity Level', fontsize=12)

# Add data labels to each bar for clarity
for bar in bars:
    width = bar.get_width()
    ax.text(width + 0.1, bar.get_y() + bar.get_height()/2, f'{int(width)}', va='center', ha='left')

# Add the rationale box to the plot
add_rationale_box(ax)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.savefig("chart_1_overall_composition.png")
print("Generated updated chart: chart_1_overall_composition.png")
plt.show()
```


=========================================
FILE: graphs/metadata_analysis_by_class.py
=========================================
```python
#!/usr/bin/env python3
"""
Generates categorical distribution plots to show how metadata properties
(outline length and depth) vary across the final classification levels.

This is a key validation chart that visually confirms the characteristics
of each structural complexity level based on its source metadata.
This version uses a vertical layout and automatic text wrapping for the rationale.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from load_results import load_classification_data
import textwrap

def add_rationale_box(fig):
    """
    Adds a detailed interpretation box to the figure.
    The text is programmatically wrapped paragraph by paragraph to ensure
    correct formatting and control over the horizontal size of the box.
    """
    WRAP_WIDTH = 25  # characters. Adjust this single value as needed.

    # --- SIMPLIFIED, MAINTAINABLE TEXT BLOCK ---
    # Use all-caps for emphasis instead of HTML tags for reliability.
    # Paragraphs are separated by a blank line.
    rationale_text = textwrap.dedent(f"""
        HOW TO INTERPRET THESE CHARTS:
        These plots validate the classification by analyzing the quality of the embedded PDF bookmarks (the machine-readable ToC).

                                     
        TOP PLOT (OUTLINE LENGTH):
                                     
        • Shows the total number of bookmarks. The log scale is essential to visualize the vast range from simple books (10-50) to large references (1000+).
                                     
        • High values signify a granular, detailed structure (e.g., Textbooks, Handbooks).

                                     
        BOTTOM PLOT (OUTLINE DEPTH):
                                     
        • Shows how deeply nested the bookmarks are (e.g., Chapter → Section → Subsection).
                                     
        • High depth (>2) is the hallmark of a Level 2 or Level 4 hierarchical book.
                                     
        • Low depth (1) indicates a flat Level 1 or Level 3 structure.

                                     
        NOTE: Level 5 books lack metadata and are clustered at 0, confirming the diagnosis.
    """).strip()

    # --- Programmatic Wrapping Logic ---
    # Split the text into paragraphs, wrap each one, then join them back
    # with double newlines for proper paragraph spacing.
    paragraphs = rationale_text.split('\n\n')
    wrapped_paragraphs = [textwrap.fill(p, width=WRAP_WIDTH) for p in paragraphs]
    final_text = '\n\n'.join(wrapped_paragraphs)

    props = dict(boxstyle='round,pad=0.5', facecolor='seashell', alpha=0.95)
    font_properties = {'family': 'monospace', 'size': 8}
    
    fig.text(0.98, 0.5, final_text, transform=fig.transFigure,
             verticalalignment='center', horizontalalignment='right',
             bbox=props, fontdict=font_properties)


def create_metadata_distribution_charts(df: pd.DataFrame):
    """
    Generates categorical distribution plots to show how metadata properties
    vary across the final classification levels.
    """
    if df.empty:
        print("DataFrame is empty. No data to plot.")
        return

    plt.style.use('seaborn-v0_8-whitegrid')
    
    fig, axes = plt.subplots(2, 1, figsize=(16, 18))
    fig.suptitle('Analysis of Metadata Properties by Final Classification Level', fontsize=24, weight='bold')

    class_order = sorted(df['classification'].unique())
    wrapped_labels = [textwrap.fill(label, 15) for label in class_order]

    # --- Plot 1: Outline Length Distribution (Top Plot) ---
    ax1 = axes[0]
    sns.stripplot(
        data=df, x='classification', y='outline_length', order=class_order,
        ax=ax1, jitter=0.3, alpha=0.7, palette='viridis',
        hue='classification', legend=False, s=8
    )
    ax1.set_yscale('log')
    ax1.set_ylim(bottom=1)
    ax1.set_title('Outline Length (Number of Bookmarks)', fontsize=18, pad=15)
    ax1.set_xlabel('', fontsize=14)
    ax1.set_ylabel('Number of Bookmarks (Log Scale)', fontsize=16)
    ax1.set_xticks(range(len(class_order)))
    ax1.set_xticklabels([])

    # --- Plot 2: Outline Depth Distribution (Bottom Plot) ---
    ax2 = axes[1]
    sns.boxplot(
        data=df, x='classification', y='outline_depth', order=class_order,
        ax=ax2, palette='plasma', hue='classification', legend=False
    )
    sns.stripplot(
        data=df, x='classification', y='outline_depth', order=class_order,
        ax=ax2, jitter=0.2, alpha=0.6, color='black'
    )
    ax2.set_title('Outline Depth (Nesting Level)', fontsize=18, pad=15)
    ax2.set_xlabel('Classification Level', fontsize=16)
    ax2.set_ylabel('Max Nesting Depth', fontsize=16)
    ax2.set_xticks(range(len(class_order)))
    ax2.set_xticklabels(wrapped_labels, fontsize=12, rotation=45, ha='right')
    max_depth = df['outline_depth'].max()
    if pd.notna(max_depth) and max_depth > 0:
        ax2.set_yticks(range(int(max_depth) + 2))
    ax2.set_ylim(bottom=-0.5)

    add_rationale_box(fig)

    # Adjust rect to reserve space on the RIGHT for the text box
    # A value of 0.8 leaves 20% of the figure width for the rationale box.
    plt.tight_layout(rect=[0, 0, 0.8, 0.95]) 
    
    output_filename = "chart_6_metadata_by_class.png"
    plt.savefig(output_filename)
    print(f"\nGenerated updated chart (with automatic wrapping): {output_filename}")
    plt.show()

if __name__ == '__main__':
    try:
        df_results = load_classification_data()
        
        df_results['classification'] = df_results['classification'].str.extract(r'(Level [1-5][AB]?)')[0]
        
        class_order = sorted(df_results['classification'].dropna().unique())
        df_results['classification'] = pd.Categorical(
            df_results['classification'], 
            categories=class_order, 
            ordered=True
        )

        create_metadata_distribution_charts(df_results)

    except FileNotFoundError:
        print("Error: 'book_classifications.jsonl' not found. Run the main pipe.py script first.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
```


=========================================
FILE: graphs/rationale.py
=========================================
```python
# graphs/rationale.py

RATIONALE = {
    "chart_1_overall_composition": """
    **Chart Rationale: Overall Corpus Composition**

    **What it Shows:** This chart provides a high-level inventory of the structural complexity across the entire book collection. Each bar represents the total number of books assigned to a specific complexity level by the AI classifier.

    **How to Interpret:**
    - **Dominant Levels:** The longest bars identify the most common structural types in your corpus. A large 'Level 2' bar indicates a collection of modern, well-structured books. A large 'Level 5' bar is a critical finding, highlighting a significant number of metadata-poor books that require advanced layout analysis.
    - **Pipeline Justification:** This view validates the design of your analysis pipeline. If both Level 2 and Level 5 are prominent, it confirms the necessity of a multi-stage approach that handles both "easy" metadata-rich cases and "hard" layout-dependent cases.
    - **Effort Estimation:** The distribution helps prioritize future parsing efforts. If most books are Level 2, refining the metadata-based parser yields the most value. If Level 5 dominates, improving the layout-analysis heuristics is key.
    """,

    "chart_2_complexity_by_category": """
    **Chart Rationale: Complexity by Book Category**

    **What it Shows:** This stacked bar chart compares the structural makeup of the 'CS_BOOKS' and 'PSY_BOOKS' collections, revealing systematic differences in publishing standards and document types between the two fields.

    **How to Interpret:**
    - **Disciplinary Patterns:** Observe the proportional differences in colors between the bars. A higher proportion of Level 2 and 4 (Hierarchical/Reference) in the CS bar is expected, reflecting the prevalence of structured technical manuals and modern textbooks.
    - **Content Age and Type:** A larger slice of Level 5 (Inferred Structure) or Level 1 (Monograph) in the PSY bar might indicate a higher prevalence of older, scanned material or narrative-driven pop-science books, which tend to have simpler or less reliable metadata.
    - **Parser Adaptability:** This chart demonstrates why a single parsing strategy would be suboptimal. The system must be flexible enough to handle the different "structural signatures" of each subject area.
    """,

    "chart_3_diagnostic_heatmap": """
    **Chart Rationale: Analysis Method vs. Final Classification (Pipeline Performance)**

    **What it Shows:** This heatmap is a crucial diagnostic tool that correlates the analysis method used ('metadata_check' or 'layout_analysis') with the final complexity level assigned to a book.

    **How to Interpret:**
    - **The "Success Diagonal":** High numbers along the diagonal from top-left to bottom-right indicate a healthy pipeline.
        - **Top-Left Quadrant:** Shows that well-structured books (Levels 1-4) were correctly identified using the fast 'metadata_check'.
        - **Bottom-Right Cell:** Shows that metadata-poor books were correctly passed to the 'layout_analysis' fallback.
    - **Investigating Anomalies:** Any significant numbers *off* this diagonal are points of interest. For example, a Level 2 book that required 'layout_analysis' might point to a PDF with faulty bookmarks that `pypdf` couldn't read, revealing a potential edge case to handle. This chart is key for debugging and improving the pipeline's efficiency.
    """,
    
    "chart_4_outline_scatter": """
    **Chart Rationale: Outline Properties vs. Structural Complexity**

    **What it Shows:** This scatter plot visualizes the "metadata signature" of books that contain a machine-readable outline (bookmarks). Each point is a book, positioned by the length (number of entries) and depth (nesting level) of its outline.

    **How to Interpret:**
    - **Structural Clusters:** Different complexity levels form distinct clusters.
        - **Level 1 (Monographs):** Typically cluster at the bottom (low depth = 1) with low-to-moderate length.
        - **Level 2 (Textbooks):** Spread across the upper-right, showing both high depth (>=2) and high length.
        - **Level 3 (Handbooks):** May appear as outliers with low depth but very high length, reflecting many top-level, un-nested chapter entries.
    - **Identifying Outliers:** A book that is far from its colored cluster is an anomaly worth investigating. This plot provides a powerful visual method for understanding the typical metadata shape of each structural category.
    """,

    "chart_5_complexity_anatomy": """
    **Chart Rationale: Anatomy of Structural Complexity Levels**

    **What it Shows:** This radar chart presents a "fingerprint" for each complexity level by plotting its average characteristics across five key scaled metrics. It provides the most comprehensive view of what defines each category.

    **How to Interpret:**
    - **Category Signatures:** Each colored polygon represents the "signature" of a complexity level.
        - **Level 5's Profile:** Will be heavily skewed towards 'Typographic Variety', with near-zero values for all metadata-related axes ('Has Bookmarks', 'Hierarchy Depth', 'ToC Length'). This is the visual definition of a book requiring layout analysis.
        - **Level 2's Profile:** Will show a large, balanced shape with high scores across all axes, representing the ideal, well-structured modern book.
        - **Level 1 vs. Level 2:** Level 1 will have a high 'Has Bookmarks' score but low 'Hierarchy Depth', clearly distinguishing it from the deeper structure of Level 2.
    - **Comparing Levels:** The shapes allow for quick visual comparison. The difference between a Level 2 textbook and a Level 4B reference manual becomes obvious through the extreme values for hierarchy and length in the latter.
    """
}
```


=========================================
FILE: graphs/load_results.py
=========================================
```python
# load_results.py
import pandas as pd
import json

def load_classification_data(filepath="../book_classifications.jsonl"):
    """
    Loads the .jsonl classification results into a pandas DataFrame.
    """
    records = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            # Flatten the nested JSON for easier analysis
            record = {
                'file_path': data['file_path'],
                'classification': data['classification_result']['classification'],
                'justification': data['classification_result']['justification'],
                'has_pypdf_outline': data['final_evidence']['has_pypdf_outline'],
                'outline_depth': data['final_evidence'].get('pypdf_outline_depth', 0),
                'outline_length': data['final_evidence'].get('pypdf_outline_length', 0),
                'analysis_type': data['final_evidence'].get('analysis_type', 'metadata_check'),
                'distinct_font_sizes': data['final_evidence'].get('distinct_font_sizes'),
                'page_number_transition': data['final_evidence'].get('page_number_style_transition_found')
            }
            # Add a top-level category column based on the file path
            record['top_level_category'] = data['file_path'].split('/')[0]
            records.append(record)
    
    df = pd.DataFrame(records)
    return df

if __name__ == '__main__':
    # Example of how to use it
    df = load_classification_data()
    print("Data loaded successfully!")
    print(df.head())
    print(f"\nDataFrame Info:")
    df.info()
```


=========================================
FILE: graphs/metadata_quality.py
=========================================
```python
import matplotlib.pyplot as plt
import seaborn as sns
from load_results import load_classification_data

def add_rationale_box(ax):
    """Adds an interpretation box to the chart."""
    rationale_text = (
        "How to Interpret This Scatter Plot:\n\n"
        "This plot visualizes the structural signature of metadata-rich books.\n\n"
        "- Level 1 (Monographs): Cluster at low depth (Y-axis), showing a flat structure.\n"
        "- Level 2/4 (Textbooks/References): Spread across high depth and high length\n"
        "  (top-right), indicating complex, nested outlines.\n"
        "- Level 3 (Handbooks): May appear with low depth but high length, suggesting\n"
        "  many top-level chapters with little internal nesting in the bookmarks."
    )
    
    props = dict(boxstyle='round,pad=0.5', facecolor='lavender', alpha=0.9)
    
    # Place text box in a region that is often sparse
    ax.text(0.5, 0.98, rationale_text, transform=ax.transAxes, fontsize=9,
            verticalalignment='top', horizontalalignment='right', bbox=props)

# Load the data and filter for books that had an outline
df = load_classification_data()
df_metadata = df[df['has_pypdf_outline'] == True].copy()

# --- Chart 4: Outline Properties vs. Complexity ---
fig, ax = plt.subplots(figsize=(12, 8))

sns.scatterplot(
    data=df_metadata,
    x='outline_length',
    y='outline_depth',
    hue='classification',
    style='classification',
    s=120,
    alpha=0.8,
    palette='deep',
    ax=ax
)

ax.set_title('Metadata Quality: Outline Properties vs. Structural Complexity', fontsize=16, pad=20)
ax.set_xlabel('Outline Length (Number of Bookmarks)', fontsize=12)
ax.set_ylabel('Outline Depth (Nesting Level)', fontsize=12)
ax.legend(title='Complexity Level')
ax.set_xscale('log')

# Add the rationale box
add_rationale_box(ax)

plt.tight_layout()
plt.savefig("chart_4_outline_scatter.png")
print("Generated updated chart: chart_4_outline_scatter.png")
plt.show()
```


=========================================
FILE: graphs/comparative_view.py
=========================================
```python
import pandas as pd
import matplotlib.pyplot as plt
from load_results import load_classification_data

def add_rationale_box(ax):
    """Adds an interpretation box to the chart."""
    rationale_text = (
        "How to Interpret This Chart:\n\n"
        "This chart compares the structural makeup of the two main book categories.\n"
        "It reveals systematic differences in publishing standards between domains.\n\n"
        "- CS_BOOKS often show more Level 2/4 (highly structured, metadata-rich)\n"
        "  due to modern digital-first publishing (e.g., LaTeX, doc generators).\n"
        "- PSY_BOOKS may have a higher share of Level 1 (monographs) and\n"
        "  Level 5 (older, scanned books with degraded structure)."
    )
    
    props = dict(boxstyle='round,pad=0.5', facecolor='ivory', alpha=0.9)
    
    # Place text box in the top left, a common empty space in bar charts
    ax.text(1.05, 0.5, rationale_text, transform=ax.transAxes, fontsize=9,
            verticalalignment='top', horizontalalignment='left', bbox=props)

# Load the data
df = load_classification_data()

# --- Chart 2: Complexity by Top-Level Category ---
# Create a cross-tabulation of counts
ct = pd.crosstab(df['top_level_category'], df['classification'])

# Ensure the columns (complexity levels) are in a logical order
complexity_order = sorted(df['classification'].unique())
ct = ct.reindex(columns=complexity_order, fill_value=0)

# Create the stacked bar chart
fig, ax = plt.subplots(figsize=(12, 8)) # Increased size
ct.plot(kind='bar', stacked=True, ax=ax, cmap='Spectral', width=0.7)

# Add labels and title
ax.set_title('Structural Complexity by Book Category (CS vs. PSY)', fontsize=16, pad=20)
ax.set_xlabel('Top-Level Category', fontsize=12)
ax.set_ylabel('Number of Books', fontsize=12)
ax.tick_params(axis='x', rotation=0)
ax.legend(title='Complexity Level', bbox_to_anchor=(1.02, 1), loc='upper left')

# Add the rationale box
add_rationale_box(ax)

plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend
plt.savefig("chart_2_complexity_by_category.png")
print("Generated updated chart: chart_2_complexity_by_category.png")
plt.show()
```


=========================================
FILE: prompt_generator.py
=========================================
```python
# prompt_generator.py (Improved)
import json

def generate_segmentation_prompt(bookmark_data: list, pdftk_metadata: list, total_pages: int, pdf_path: str) -> str:
    """
    Constructs a detailed, structured, and unambiguous prompt for the Gemini API 
    to request pdftk commands for segmenting a book into individual components.
    """
    # This prompt has been heavily revised for clarity and to prevent chapter aggregation.
    prompt_rules = """
You are a master PDF document analyst. Your sole task is to generate a JSON object containing a list of `pdftk` command-line instructions. These commands will segment a book PDF into its core components based on the provided bookmark data (Table of Contents).

**Your Primary Goal:**
Analyze the bookmark data to identify the precise start and end page for each distinct component of the book.

**Required Components to Identify (if present):**
- Title Page (Mandatory, almost always page 1)
- Table of Contents (Mandatory)
- Foreword
- Preface
- Dedication
- Acknowledgments
- **Each individual Chapter** (CRITICAL: Every chapter must be a separate entry)
- **Each individual Appendix** (CRITICAL: Every appendix must be a separate entry)
- Glossary
- Bibliography / References
- Index

---
**CRITICAL OUTPUT RULES**
---
1.  **JSON ONLY:** Your entire response MUST be a single, valid JSON object. Do not wrap it in markdown, code ticks, or add any explanatory text outside of the JSON structure itself.
2.  **ROOT STRUCTURE:** The JSON object must have a single root key named `"segmentation_commands"`, which contains a list of command objects.
3.  **COMMAND OBJECT STRUCTURE:** Each object in the `"segmentation_commands"` list must have exactly three string keys:
    - `"component_name"`: A file-safe name for the output PDF. Use leading zeros for sorting (e.g., "00_Title_Page", "01_Table_of_Contents", "05_Chapter_03_Methodology").
    - `"pdftk_command"`: The precise `pdftk IN_FILE cat START-END output OUT_FILE` command. You must use the literal placeholders "IN_FILE" and "OUT_FILE".
    - `"justification"`: A brief, single-sentence explanation of how you determined the page range using the bookmark titles.

4.  **CRITICAL RULE - NO CHAPTER AGGREGATION:** You **MUST** generate a separate command object for each individual chapter. For instance, if you identify "Chapter 1", "Chapter 2", and "Chapter 3", you must output three distinct command objects in the list. **NEVER group chapters** (e.g., "Chapters 1-3") into a single command. This is the most important rule.

5.  **PAGE RANGE LOGIC:**
    - The end page of a component is the page number immediately before the start page of the very next component.
    - The very last component in the book (e.g., the final chapter or appendix) must end at the `total_pages` number.
    - Infer components like a "Title Page" or "Copyright Page" from the page gap between the start of the PDF and the first major bookmark (like "Table of Contents").

6.  **BE CONSERVATIVE:** If the provided bookmark data is too sparse, ambiguous, or corrupted to confidently determine the page ranges for the mandatory components, you MUST return an empty list for the `"segmentation_commands"` value. It is better to fail safely (`"segmentation_commands": []`) than to guess and produce incorrect segments.

---
**INPUT DATA**
---

**1. Total Pages in PDF:**
{total_pages}

**2. Bookmark Data (from PyPDF or similar):**
```json
{bookmark_json_string}
```

**3. Pdftk Metadata (additional context):**
```json
{pdftk_json_string}
```
"""
    return prompt_rules.format(
        total_pages=total_pages,
        bookmark_json_string=json.dumps(bookmark_data, indent=2),
        pdftk_json_string=json.dumps(pdftk_metadata, indent=2)
        # pdf_path is not needed in the prompt itself, only for the final command execution
    )
```


=========================================
FILE: StructuralComplexityClassificationSystem.md
=========================================
```markdown
# **Comprehensive Report on Book Structural Complexity Levels (Revised)**

## **1. Introduction**

This report provides a detailed specification for the hierarchical classification system used to categorize the structural complexity of digital books in the provided corpus. The system now includes all specified variations to account for a wider range of document quality and structure.

The classification for each book is determined by a two-stage analysis pipeline:
1.  **Stage 1 (`metadata_checker.py`):** A fast check for explicit structural metadata (PDF bookmarks).
2.  **Stage 2 (`layout_analyzer.py`):** A fallback analysis of the document's visual layout for books that fail Stage 1.

The evidence is then synthesized by a generative AI (`ai_classifier.py`) to assign a final complexity level.

---

## **2. The Complexity Hierarchy (Complete)**

### **Level 1A: Simple Linear Monograph (with High-Quality Metadata)**

*   **Definition:** A book with a flat or very shallow chapter structure, minimal formatting variation, and a primary focus on linear, narrative text. It is structurally simple and possesses a clean, machine-readable outline.
*   **Key Characteristics:**
    *   **Structure:** Sequential, un-nested chapters.
    *   **Hierarchy:** Flat. The main structural unit is the chapter.
    *   **Content:** Dominated by long-form prose.
    *   **Metadata:** Contains a complete and accurate set of bookmarks corresponding to the chapter titles.
*   **Empirical Features & Data Signature:**
    *   `analysis_type`: `metadata_check`.
    *   `has_pypdf_outline`: `True`.
    *   `outline_length`: Low to Moderate (typically 10-40 entries).
    *   `outline_depth`: **1**.
    *   `page_number_style_transition_found`: Possible.
*   **Parsing Strategy:** Reliably parsed using the ToC (bookmarks). A simple loop over the outline entries is sufficient.
*   **Canonical Examples from Corpus:** `mindsetTheNewPsychologyOfSucess.pdf`, `DeepWork-RulesforFocusedSuccessinaDistractedWorld.pdf`.

### **Level 1B: Simple Linear Monograph (with Ambiguous or Incomplete Metadata)**

*   **Definition:** A book that is structurally a simple monograph, but its digital metadata is flawed, incomplete, or non-existent, forcing a partial or full reliance on layout analysis.
*   **Key Characteristics:**
    *   **Structure:** Logically flat and sequential, like Level 1A.
    *   **Metadata:** The bookmarks may be missing, cover only a few chapters, or be present but malformed (e.g., all pointing to the first page). The book may fail the initial metadata check.
    *   **Appearance:** Visually, the book is simple, but this simplicity is not reflected in its machine-readable structure.
*   **Empirical Features & Data Signature:**
    *   `analysis_type`: Can be either `metadata_check` (if metadata is present but useless) or `layout_analysis` (if metadata is absent).
    *   `has_pypdf_outline`: Can be `True` or `False`.
    *   `outline_length`: If `True`, the length is often suspiciously low for the book's page count.
    *   `outline_depth`: If `True`, the depth is 1.
    *   `distinct_font_sizes`: If layout analysis is run, this will be the primary clue. A low number of distinct font sizes (e.g., one for body, one for headings) suggests a simple structure.
*   **Parsing Strategy:** The pipeline must be robust. If initial parsing based on a weak outline fails to yield a sensible structure (e.g., one 300-page "chapter"), the system must fall back to the Level 5 layout-based strategy, but constrained by the knowledge that it's likely looking for a simple, flat hierarchy.
*   **Canonical Examples from Corpus:** An older book like `HowToWinFriendsAndInfluencePeople.pdf` is a prime candidate if it's a scan that had partial, auto-generated bookmarks added later. No examples are explicitly this type in the `jsonl` yet, as the final classification depends on the success of the chosen analysis method.

---

### **Level 2A: Standard Hierarchical Textbook (with High-Quality Metadata)**

*   **Definition:** A modern, well-structured book with a consistent and deeply nested hierarchy, fully and accurately represented by its PDF bookmarks.
*   **Key Characteristics:**
    *   **Structure:** Clear hierarchy of Parts, Chapters, Sections, and Sub-sections.
    *   **Hierarchy:** Deep and predictable, with a systematic numbering scheme (e.g., 2.1, 2.1.1).
    *   **Metadata:** A comprehensive and accurate outline that mirrors the visual ToC.
*   **Empirical Features & Data Signature:**
    *   `analysis_type`: `metadata_check`.
    *   `has_pypdf_outline`: `True`.
    *   `outline_length`: Moderate to High (50-500 entries).
    *   `outline_depth`: **Greater than 1 (usually 2 to 4)**.
    *   `page_number_style_transition_found`: Highly likely.
*   **Parsing Strategy:** Reliably parsed using a recursive traversal of the bookmark outline.
*   **Canonical Examples from Corpus:** `DesigningData-IntensiveApplications_MartinKleppmann_2017.pdf`, `Thinking-FastAndSlow.pdf`.

### **Level 2B: Hierarchical Textbook (with Inconsistent or Non-Standard Structure)**

*   **Definition:** A book that is intended to be hierarchical, but its structure is inconsistent, uses non-standard numbering, or is poorly reflected in its metadata. It is more complex to parse than a standard textbook but is not a composite work like a handbook.
*   **Key Characteristics:**
    *   **Structure:** Attempts a hierarchy, but may lack consistency. For example, some chapters are deeply nested while others are flat.
    *   **Numbering:** May mix schemes (e.g., `Chapter 1, Section A, Subsection 1.2.c`) or lack numbering for some headings.
    *   **Metadata:** The bookmarks might be present but messy, omitting certain levels of the hierarchy or representing it inaccurately.
*   **Empirical Features & Data Signature:**
    *   `analysis_type`: `metadata_check`.
    *   `has_pypdf_outline`: `True`.
    *   `outline_length` and `outline_depth`: Highly variable. The key signal is the *inconsistency* which is not captured by a single number but would be apparent in the full outline data. The AI must infer this from the justification based on title patterns.
*   **Parsing Strategy:** This is a challenging case. The parser must start with the provided outline but cannot fully trust it. It should use the outline as a set of "strong hints" for chapter boundaries and then apply a localized layout analysis *within* each chapter to find the true sub-structure. This requires a hybrid approach.
*   **Canonical Examples from Corpus:** Could include older textbooks from the early digital era or modern self-published books where editorial standards were less rigorous. The file `IntroductiontoCalculusandAnalysisVolI_RichardCourant-FritzJohn.pdf` (originally from 1965) could potentially fit this if its digital version has an imperfectly reconstructed outline.

---

### **Level 3: Composite Edited Handbook/Collection**

*   **Definition:** A collection of distinct articles, essays, or chapters, often by different authors, compiled by an editor(s). While structurally sound at the chapter level, the internal structure of each chapter is inconsistent.

*   **Key Characteristics:**
    *   **Structure:** A collection of self-contained units presented as chapters. Often grouped into "Parts" or "Sections."
    *   **Hierarchy:** The ToC is typically flat at the chapter level or has one level of grouping (Part -> Chapter). The internal structure of each chapter (sections, subsections) varies and is often not reflected in the main ToC.
    *   **Consistency:** The key challenge. Formatting, numbering, and reference styles can differ from one chapter to the next.
    *   **Paratext:** The ToC is the primary unifying element. Each chapter may have its own bibliography.

*   **Empirical Features & Data Signature:**
    *   `analysis_type`: `metadata_check`.
    *   `has_pypdf_outline`: `True`.
    *   `outline_length`: **High to Very High** (often 100+ entries) due to the large number of distinct contributions.
    *   `outline_depth`: **Low (typically 1 or 2)**. This combination of high length and low depth is a strong signal for this category.

*   **Parsing Strategy:**
    *   The ToC is used to determine chapter boundaries.
    *   To parse *within* a chapter (sub-chapters), a secondary, layout-based analysis would be required on a per-chapter basis, as the global structure is unreliable.

*   **Canonical Examples from Corpus:**
    *   `PSY_BOOKS/CREATIVITY/the-cambridge-handbook-of-creativity.pdf`
    *   `PSY_BOOKS/PERSONALITY/HandbookOfPersonality2008.pdf`
    *   `CS_BOOKS/DataEngineering&Systems/ReadingsInDatabaseSystems_Bailis-Hellerstein-Stonebraker_2015.pdf`

---

### **Level 4A: Hierarchical with Asymmetric Appendices**

*   **Definition:** A Level 2 book that contains large, structurally distinct back-matter, such as appendices, glossaries, or reference tables, which do not follow the primary hierarchical pattern of the main body.

*   **Key Characteristics:**
    *   **Structure:** A core of a standard hierarchical textbook, followed by one or more large sections with a different purpose and format.
    *   **Asymmetry:** The structural rules for the main body do not apply to the back-matter. For example, appendices may be a flat list, while the main content is deeply nested.
    *   **Numbering:** Often switches schemes (e.g., Chapters 1-12, Appendix A, Appendix B).

*   **Empirical Features & Data Signature:**
    *   `analysis_type`: `metadata_check`.
    *   `has_pypdf_outline`: `True`.
    *   `outline_length` and `outline_depth`: Similar to Level 2 (moderate to high).
    *   The distinction is not purely quantitative but relies on semantic cues in the ToC titles (e.g., "Appendix," "Glossary," "Bibliography") that signal a change in structure. The AI's pattern recognition is key here.

*   **Parsing Strategy:**
    *   A state-machine approach is required. The parser uses the ToC to identify the boundaries of these major sections and applies different rule sets accordingly (e.g., hierarchical parsing for the body, simpler list parsing for appendices).

*   **Canonical Examples from Corpus:**
    *   `PSY_BOOKS/INTELLIGENCE/PSYCHOMETRIC_ASSESSMENT/EssentialsOfWAIS-IVAssessment-2ndEdition.pdf`
    *   `CS_BOOKS/FoundationalMathematics&Statistics/ConcreteMathematics_Graham-Knuth-Patashnik_1994.pdf`

---

### **Level 4B: Modular Reference Collection**

*   **Definition:** Not a single narrative book, but a collection of distinct, self-contained documents (e.g., a tutorial, a library reference, an API guide) bundled into a single PDF.

*   **Key Characteristics:**
    *   **Structure:** A container of multiple, independently structured documents.
    *   **Hierarchy:** Often exhibits **extremely deep and long outlines**, as each sub-document may have its own detailed, multi-level ToC.
    *   **Purpose:** Serves as a comprehensive reference manual rather than a book to be read linearly.

*   **Empirical Features & Data Signature:**
    *   `analysis_type`: `metadata_check`.
    *   `has_pypdf_outline`: `True`.
    *   `outline_length`: **Very High to Extremely High** (can exceed 1000+ entries).
    *   `outline_depth`: **Very High** (can be 4+ levels deep).
    *   The sheer scale of the outline is the primary quantitative indicator.

*   **Parsing Strategy:**
    *   Requires a sophisticated state-machine or modular parsing approach. The top-level outline entries (e.g., "Tutorial", "Library Reference", "C API") define the boundaries between different document types, each requiring its own specialized parsing logic.

*   **Canonical Examples from Corpus:**
    *   `CS_BOOKS/Programming&CoreComputerScience/python-3.13-docs-pdf-a4/` (The entire directory of PDFs represents this class). The `tutorial.txt` metadata file is a prime example of its deep, long outline.
    *   `CS_BOOKS/Programming&CoreComputerScience/RUST/TheRustReference1.88.0.pdf`

---

### **Level 5A: Corrupt or Misleading Metadata**

*   **Definition:** A book that *appears* to have metadata (passes Stage 1), but the metadata is fundamentally unusable or misleading (e.g., nonsensical titles, all bookmarks pointing to page 1, an outline for a completely different book). This is a "false positive" from the metadata check.
*   **Key Characteristics:**
    *   **Metadata:** Present but garbage. It leads the parser to an incorrect or failed deconstruction of the book.
    *   **Origin:** Often a result of poor automated processing by a digital library or a faulty PDF creation tool.
*   **Empirical Features & Data Signature:**
    *   `analysis_type`: **`metadata_check`**.
    *   `has_pypdf_outline`: **`True`**.
    *   The `outline_length` and `outline_depth` could be anything, which is what makes this case deceptive.
    *   This is a diagnostic failure case. The initial data signature looks like Level 1-4, but the subsequent parsing attempt would fail.
*   **Parsing Strategy:** This requires a validation step in the pipeline. After an initial parse based on bookmarks, the system should run a quick sanity check (e.g., "Do all chapters have a non-zero page range?"). If the check fails, the book must be **re-queued for a full Level 5B layout analysis**, overriding the initial evidence.
*   **Canonical Examples from Corpus:** No book is classified this way because it represents a pipeline failure, not a final state. A hypothetical example would be a file where `extract_chapter.py` returns a long list of bookmarks, but all have `"page": 1`.

### **Level 5B: Degraded or Typographically Inferred Structure (No Metadata)**

*   **Definition:** The "true" Level 5. A book that completely lacks any machine-readable metadata (bookmarks). Its logical structure must be inferred entirely from its visual layout and typography.
*   **Key Characteristics:**
    *   **Metadata:** Complete absence of a usable outline.
    *   **Origin:** Often older, scanned books or simple digital transcriptions where structural information was never added.
*   **Empirical Features & Data Signature:**
    *   `analysis_type`: **`layout_analysis`**.
    *   `has_pypdf_outline`: **`False`**.
    *   `outline_length`: `0`.
    *   `outline_depth`: `0`.
    *   `distinct_font_sizes`: High. This typographic variety is the primary signal for inference.
    *   `page_number_style_transition_found`: A very strong corroborating signal if `True`.
*   **Parsing Strategy:** The pure layout analysis path described previously. The system profiles typography, identifies candidate headings based on deviations from the norm, and reconstructs a plausible hierarchy from these visual cues.
*   **Canonical Examples from Corpus:** Any book in the collection that fails the `metadata_checker.py` script and is passed to `layout_analyzer.py` will be a candidate for this classification. The AI's final decision will be based on the layout evidence provided.
```


=========================================
FILE: extract_chapter.py
=========================================
```python
# extract_chapter.py (Fully Enhanced with Retries, Fallbacks, and Resilience)

#!/usr/bin/env python3
"""
extract_chapter.py
Extracts and prints the outline (bookmarks/chapters) from a given PDF file.
Enhanced with retries, PyMuPDF fallback for page count, improved password handling,
and full resilience against timeouts/errors.
"""

import argparse
import sys
import pypdf
import subprocess
import os
import time
import fitz  # PyMuPDF for fallback page count

def get_page_count_fallback(pdf_path: str) -> int:
    """
    Fallback page count using PyMuPDF (fitz).
    """
    try:
        doc = fitz.open(pdf_path)
        return len(doc)
    except Exception as e:
        print(f"  -> WARNING: PyMuPDF fallback failed for '{pdf_path}': {e}", file=sys.stderr)
        return 0

def get_page_count(pdf_path: str, max_retries: int = 3) -> int:
    """
    Gets the total number of pages from a PDF using pdftk with retries.
    Falls back to PyMuPDF if all retries fail. Resilient to errors.
    """
    abs_pdf_path = os.path.abspath(pdf_path)
    for retry in range(max_retries):
        try:
            # Use binary capture to avoid internal thread decode errors
            result = subprocess.run(
                ["pdftk", abs_pdf_path, "dump_data_utf8"],
                capture_output=True,
                check=True,
                timeout=120  # Increased timeout
            )
            # Manually decode stdout with 'replace' to handle invalid bytes
            stdout_decoded = result.stdout.decode('utf-8', errors='replace')
            for line in stdout_decoded.splitlines():
                if line.startswith("NumberOfPages:"):
                    return int(line.split(":")[1].strip())
            # If no NumberOfPages, fallback immediately
            print(f"  -> No page count in pdftk output for '{pdf_path}'. Falling back to PyMuPDF.")
            return get_page_count_fallback(pdf_path)
        except subprocess.TimeoutExpired:
            if retry < max_retries - 1:
                wait = min(2 ** retry, 15)
                print(f"  -> pdftk timeout (retry {retry+1}/{max_retries}), waiting {wait}s...")
                time.sleep(wait)
                continue
            else:
                print(f"  -> All pdftk retries timed out for '{pdf_path}'. Falling back to PyMuPDF.")
                return get_page_count_fallback(pdf_path)
        except Exception as e:
            print(f"  -> pdftk error (retry {retry+1}/{max_retries}) for '{pdf_path}': {e}", file=sys.stderr)
            if retry == max_retries - 1:
                print(f"  -> All pdftk retries failed. Falling back to PyMuPDF.")
                return get_page_count_fallback(pdf_path)
            time.sleep(2 ** retry)
    return 0

def remove_pdf_password_if_needed(pdf_path, max_retries: int = 3):
    """
    Checks if the PDF has an owner password by attempting a pdftk dump_data_utf8 with retries.
    If password detected, uses Ghostscript (with verification), falls back to qpdf.
    Handles timeouts/non-zero exits gracefully.
    """
    # Convert to absolute path to avoid relative path issues
    abs_pdf_path = os.path.abspath(pdf_path)
    
    for retry in range(max_retries):
        try:
            # Run without check=True to handle non-zero exits manually
            result = subprocess.run(
                ["pdftk", abs_pdf_path, "dump_data_utf8"],
                capture_output=True,
                timeout=120  # Increased timeout
            )
            # Manually decode stderr for error check
            stderr_decoded = result.stderr.decode('utf-8', errors='replace')
            if result.returncode == 0:
                # If successful, no password issue
                return False
            if "OWNER PASSWORD REQUIRED" in stderr_decoded:
                print(f"  -> PDF '{pdf_path}' has owner password. Attempting removal with Ghostscript...")
                
                # Get original page count first (with fallback)
                original_pages = get_page_count(pdf_path)
                if original_pages == 0:
                    print(f"  -> Could not get original page count for '{pdf_path}'. Skipping decryption.", file=sys.stderr)
                    return False
                
                # Try Ghostscript first
                try:
                    unprotected_path = abs_pdf_path.replace('.pdf', '_unprotected.pdf')
                    gs_cmd = [
                        "gswin64c",
                        "-sPDFPassword=",
                        "-dNOPAUSE",
                        "-dBATCH",
                        "-sDEVICE=pdfwrite",
                        f"-sOutputFile=\"{unprotected_path}\"",
                        "-f",
                        f"\"{abs_pdf_path}\""
                    ]
                    gs_result = subprocess.run(gs_cmd, capture_output=True, check=True, timeout=300)  # Longer for GS
                    if gs_result.returncode == 0:
                        new_pages = get_page_count(unprotected_path)
                        if new_pages == original_pages and new_pages > 0:
                            os.replace(unprotected_path, abs_pdf_path)
                            print(f"  -> Successfully removed password from '{pdf_path}' with Ghostscript (pages: {original_pages}).")
                            return True
                        else:
                            print(f"  -> Ghostscript output invalid (pages: {new_pages} vs {original_pages}). Falling back to qpdf.")
                            if os.path.exists(unprotected_path):
                                os.remove(unprotected_path)
                    else:
                        print(f"  -> Ghostscript failed: {gs_result.stderr.decode('utf-8', errors='replace')}. Falling back to qpdf.")
                except subprocess.TimeoutExpired:
                    print("  -> Ghostscript timed out. Falling back to qpdf.")
                except Exception as gs_e:
                    print(f"  -> Ghostscript error: {gs_e}. Falling back to qpdf.")
                
                # Fallback: qpdf
                print("  -> Attempting qpdf...")
                try:
                    qpdf_unprotected_path = abs_pdf_path.replace('.pdf', '_qpdf_unprotected.pdf')
                    qpdf_cmd = ["qpdf", "--decrypt", f"\"{abs_pdf_path}\"", f"\"{qpdf_unprotected_path}\""]
                    qpdf_result = subprocess.run(qpdf_cmd, capture_output=True, check=True, timeout=120)
                    if qpdf_result.returncode == 0:
                        new_pages = get_page_count(qpdf_unprotected_path)
                        if new_pages == original_pages and new_pages > 0:
                            os.replace(qpdf_unprotected_path, abs_pdf_path)
                            print(f"  -> Successfully removed password from '{pdf_path}' with qpdf (pages: {original_pages}).")
                            return True
                        else:
                            print(f"  -> qpdf output invalid (pages: {new_pages} vs {original_pages}).")
                            if os.path.exists(qpdf_unprotected_path):
                                os.remove(qpdf_unprotected_path)
                    else:
                        print(f"  -> qpdf failed: {qpdf_result.stderr.decode('utf-8', errors='replace')}.")
                except subprocess.TimeoutExpired:
                    print("  -> qpdf timed out.")
                except Exception as qpdf_e:
                    print(f"  -> qpdf error: {qpdf_e}.")
                
                print(f"  -> Decryption failed after retries. Skipping for '{pdf_path}'.")
                return False
            else:
                # Non-password error: Log and skip
                print(f"  -> pdftk non-password error (code {result.returncode}, retry {retry+1}): {stderr_decoded}")
                if retry == max_retries - 1:
                    return False
                time.sleep(2 ** retry)
                continue
        except subprocess.TimeoutExpired:
            if retry < max_retries - 1:
                wait = min(2 ** retry, 15)
                print(f"  -> pdftk timeout in password check (retry {retry+1}/{max_retries}), waiting {wait}s...")
                time.sleep(wait)
                continue
            else:
                print(f"  -> All password check retries timed out. Skipping.")
                return False
        except Exception as e:
            print(f"  -> Unexpected password check error (retry {retry+1}): {e}")
            if retry == max_retries - 1:
                return False
            time.sleep(2 ** retry)
    return False

def get_chapter_data(pdf_path):
    """
    Reads a PDF file and extracts its outline (bookmarks) using PyPDF (primary, fast).
    """
    try:
        reader = pypdf.PdfReader(pdf_path)
    except pypdf.errors.PdfReadError as e:
        print(f"Error: Could not read the PDF file. It might be corrupted or encrypted.", file=sys.stderr)
        print(f"Details: {e}", file=sys.stderr)
        return []

    chapter_data = []
    outlines = reader.outline

    if not outlines:
        return []

    # Recursive function to process the nested outline structure
    def process_outline_item(item, level=0):
        if isinstance(item, pypdf.generic.Destination):
            try:
                page_number = reader.get_page_number(item.page) + 1
                chapter_data.append({
                    "title": item.title,
                    "page": page_number,
                    "level": level
                })
            except Exception:
                pass
        elif isinstance(item, list):
            for sub_item in item:
                process_outline_item(sub_item, level + 1)

    for item in outlines:
        process_outline_item(item, level=0)

    return chapter_data

def get_pdftk_metadata(pdf_path, max_retries: int = 3):
    """
    Extracts bookmark metadata using pdftk with retries (secondary to PyPDF).
    If fails, returns empty list (use PyPDF as primary).
    """
    # Handle password first
    password_removed = remove_pdf_password_if_needed(pdf_path)
    if password_removed:
        print("  -> Password removed; proceeding with metadata extraction.")

    # Convert to absolute path
    abs_pdf_path = os.path.abspath(pdf_path)
    
    for retry in range(max_retries):
        try:
            # Use binary capture
            result = subprocess.run(
                ["pdftk", abs_pdf_path, "dump_data_utf8"],
                capture_output=True,
                check=True,
                timeout=120
            )
            # Manually decode
            stdout_decoded = result.stdout.decode('utf-8', errors='replace')
            lines = stdout_decoded.splitlines()
            chapter_data = []
            current_bookmark = {}

            for line in lines:
                if line.startswith("BookmarkBegin"):
                    current_bookmark = {}
                elif line.startswith("BookmarkTitle: "):
                    current_bookmark["title"] = line[len("BookmarkTitle: "):]
                elif line.startswith("BookmarkLevel: "):
                    try:
                        current_bookmark["level"] = int(line[len("BookmarkLevel: "):]) - 1
                    except ValueError:
                        continue
                elif line.startswith("BookmarkPageNumber: "):
                    try:
                        current_bookmark["page"] = int(line[len("BookmarkPageNumber: "):])
                    except ValueError:
                        continue
                    if "title" in current_bookmark and "page" in current_bookmark and "level" in current_bookmark:
                        chapter_data.append(current_bookmark)

            return chapter_data
        except subprocess.TimeoutExpired:
            if retry < max_retries - 1:
                wait = min(2 ** retry, 15)
                print(f"  -> pdftk timeout in metadata (retry {retry+1}/{max_retries}), waiting {wait}s...")
                time.sleep(wait)
                continue
            else:
                print("  -> All pdftk metadata retries failed. Using PyPDF only.")
                return []  # Fallback to empty; use PyPDF in caller
        except Exception as e:
            print(f"  -> pdftk metadata error (retry {retry+1}): {e}")
            if retry == max_retries - 1:
                print("  -> All retries failed. Using PyPDF only.")
                return []
            time.sleep(2 ** retry)
    return []

# Main function unchanged
def main():
    parser = argparse.ArgumentParser(
        description="Extracts and prints the chapter outline from a PDF file.",
        formatter_class=argparse.RawTextFormatter,
        epilog="Example:\n  python3 %(prog)s 'DesigningData-IntensiveApplications.pdf'"
    )
    parser.add_argument(
        "pdf_filepath",
        type=str,
        help="The path to the PDF file to process."
    )
    args = parser.parse_args()

    try:
        chapters = get_chapter_data(args.pdf_filepath)
    except FileNotFoundError:
        print(f"Error: The file '{args.pdf_filepath}' was not found.", file=sys.stderr)
        sys.exit(1)

    if chapters is None:
        sys.exit(1)

    if chapters:
        print(f"Chapter Data for '{args.pdf_filepath}':")
        for chapter in chapters:
            indent = "  " * chapter["level"]
            print(f"{indent}Title: {chapter['title']}, Page: {chapter['page']}")
    else:
        print(f"No outline/bookmark data found in '{args.pdf_filepath}'.")

if __name__ == "__main__":
    main()
```


=========================================
FILE: layout_analyzer.py
=========================================
```python
#!/usr/bin/env python3
import fitz  # PyMuPDF
import json
import argparse
import sys
from collections import Counter
import re

def analyze_page_number_style(page_text):
    """Detects Roman or Arabic numerals in the last part of page text."""
    # A simple regex to find numbers at the end of a string
    arabic_match = re.search(r'\b(\d{1,4})\b\s*$', page_text)
    roman_match = re.search(r'\b([ivxlcdm]+)\b\s*$', page_text.lower())

    if arabic_match:
        return "arabic"
    if roman_match:
        return "roman"
    return "none"

def analyze_book_layout(pdf_path, max_pages=50):
    """
    Performs Stage 2 layout analysis on a PDF.
    """
    print(f"--- Performing Layout Analysis for: {pdf_path} ---")
    try:
        doc = fitz.open(pdf_path)
    except Exception as e:
        print(f"Error opening PDF with PyMuPDF: {e}", file=sys.stderr)
        return None

    font_sizes = Counter()
    page_number_styles = []
    num_pages_to_scan = min(len(doc), max_pages)

    for i in range(num_pages_to_scan):
        page = doc.load_page(i)
        
        # 1. Profile font sizes
        blocks = page.get_text("dict")["blocks"]
        for block in blocks:
            if "lines" in block:
                for line in block["lines"]:
                    for span in line["spans"]:
                        font_sizes[round(span["size"])] += 1
                        
        # 2. Detect page numbering style
        full_text = page.get_text().strip()
        if full_text:
            # Check the last 50 chars for a page number
            style = analyze_page_number_style(full_text[-50:])
            if style != "none":
                page_number_styles.append(style)

    # 3. Synthesize findings
    num_distinct_font_sizes = len(font_sizes)
    most_common_fonts = font_sizes.most_common(5)
    
    # Check for the key transition from Roman to Arabic numerals
    transition_found = False
    if 'roman' in page_number_styles and 'arabic' in page_number_styles:
        try:
            last_roman_idx = len(page_number_styles) - 1 - page_number_styles[::-1].index('roman')
            first_arabic_idx = page_number_styles.index('arabic')
            if first_arabic_idx > last_roman_idx:
                transition_found = True
        except ValueError:
            pass # Should not happen if both are in list

    evidence = {
        "file": pdf_path,
        "analysis_type": "layout_analysis",
        "distinct_font_sizes": num_distinct_font_sizes,
        "top_5_font_sizes": most_common_fonts,
        "page_number_style_transition_found": transition_found,
        "detected_page_number_styles": list(dict.fromkeys(page_number_styles)) # unique styles found
    }
    
    print("-> Layout analysis complete.")
    return evidence

def main():
    parser = argparse.ArgumentParser(description="Stage 2: Analyze PDF layout for structural clues.")
    parser.add_argument("pdf_filepath", type=str, help="The path to the PDF file.")
    args = parser.parse_args()

    try:
        evidence = analyze_book_layout(args.pdf_filepath)
        if evidence:
            print("\n--- Layout Evidence Summary ---")
            print(json.dumps(evidence, indent=2))
    except FileNotFoundError:
        print(f"Error: The file '{args.pdf_filepath}' was not found.", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()

```


=========================================
FILE: ai_classifier.py
=========================================
```python
# ai_classifier.py
# (This uses the get_gemini_response function from your template)
import json

def generate_classification_prompt(evidence: dict) -> str:
    """Creates a detailed prompt for the AI classifier."""
    
    hierarchy_definitions = """
    **Level 1: Simple Linear Monograph:** Flat chapter structure, minimal formatting changes.
    **Level 2: Standard Hierarchical Textbook:** Consistent, deep hierarchy (e.g., 1.1, 1.1.1), predictable formatting.
    **Level 3: Composite Edited Handbook/Collection:** Chapters by different authors; inconsistent internal structure per chapter.
    **Level 4A: Hierarchical with Asymmetric Appendices:** A Level 2 book with large, structurally different back-matter (Appendices, Glossary).
    **Level 4B: Modular Reference Collection:** A bundle of separate manuals (e.g., tutorial, API reference), not one book.
    **Level 5: Degraded or Typographically Inferred Structure:** Lacks explicit metadata (bookmarks). Structure must be inferred from layout alone (font sizes, spacing).
    """

    prompt = f"""
    Please act as an expert in computational document analysis. Your task is to classify the structural complexity of a book based on the evidence gathered by analysis scripts.

    **Book File:**
    {evidence.get('file')}

    **Structural Complexity Hierarchy:**
    {hierarchy_definitions}

    **Evidence Collected:**
    ```json
    {json.dumps(evidence, indent=2)}
    ```

    **Analysis and Classification Task:**
    Based *only* on the evidence provided, assign the book to the most appropriate structural complexity level from the hierarchy. Provide a single-line justification for your choice.

    **Format your response as follows:**
    LEVEL: [Your chosen level, e.g., Level 2]
    JUSTIFICATION: [Your single-line justification]
    """
    return prompt

```


=========================================
FILE: metadata_checker.py
=========================================
```python
# metadata_checker.py (Enhanced with Retries and Fallbacks)

# metadata_checker.py (Enhanced with Password Cache)

#!/usr/bin/env python3
import json
import argparse
import sys
from extract_chapter import get_chapter_data, remove_pdf_password_if_needed, get_pdftk_metadata

# Quick Fix 3: Password Cache (global for module)
password_cache = {}

def get_cached_password_removal(pdf_path):
    if pdf_path not in password_cache:
        password_cache[pdf_path] = remove_pdf_password_if_needed(pdf_path)
    return password_cache[pdf_path]

def check_book_metadata(pdf_path):
    """
    Performs Stage 1 analysis using PyPDF (primary) and pdftk (secondary) with retries.
    """
    print(f"--- Analyzing Metadata for: {pdf_path} ---")
    
    # Quick Fix 3: Cached password removal
    password_removed = get_cached_password_removal(pdf_path)
    if password_removed:
        print("-> Password removed (cached); proceeding.")
    
    bookmarks = get_chapter_data(pdf_path)  # PyPDF primary
    
    try:
        # Use enhanced get_pdftk_metadata with retries
        pdftk_metadata = get_pdftk_metadata(pdf_path)
        has_pdftk_metadata = bool(pdftk_metadata)
    except Exception as e:
        print(f"-> WARNING: pdftk failed for '{pdf_path}': {e}")
        has_pdftk_metadata = False

    evidence = {
        "file": pdf_path,
        "analysis_type": "metadata_check",
        "has_pypdf_outline": bool(bookmarks),
        "pypdf_outline_depth": max(b['level'] for b in bookmarks) + 1 if bookmarks else 0,
        "pypdf_outline_length": len(bookmarks),
        "has_pdftk_metadata": has_pdftk_metadata,
    }

    if not bookmarks and not has_pdftk_metadata:
        print("-> Metadata check FAILED. Proceed to layout.")
        evidence["next_step"] = "run_layout_analysis"
    else:
        print("-> Metadata check PASSED.")
        evidence["next_step"] = "classify_with_ai"

    return evidence

def main():
    parser = argparse.ArgumentParser(description="Stage 1: Check PDF for explicit metadata.")
    parser.add_argument("pdf_filepath", type=str, help="The path to the PDF file.")
    args = parser.parse_args()
    
    try:
        evidence = check_book_metadata(args.pdf_filepath)
        print("\n--- Evidence Summary ---")
        evidence.pop('next_step', None)
        print(json.dumps(evidence, indent=2))
    except FileNotFoundError:
        print(f"Error: File '{args.pdf_filepath}' not found.", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()
```


=========================================
FILE: diagrams/diagram.puml
=========================================
```plantuml
@startuml
title **PDF Segmentation Pipeline Architecture**
header LLM-Powered Command Generation
footer System Workflow - Version 1.0

skinparam backgroundColor #FDFDFD
skinparam handwriting true
skinparam defaultFontName "Apex"
skinparam activity {
  BorderColor #555555
  BackgroundColor LightGreen
  ArrowColor #333333
}
skinparam partition {
  BackgroundColor #F0F0F0
  BorderColor #AAAAAA
}
skinparam note {
  BackgroundColor #FFEFE0
  BorderColor #C0C000
}

start

partition "segmentation_pipe.py (Orchestrator)" {
  :Read 'book_classifications.jsonl';
  :Filter for 'safe' books\n(analysis_type == "metadata_check");
  if (Any books were skipped?) then (yes)
    partition "File System" {
      :Log skipped book paths to 'logs/skipped_books.txt';
    }
  endif
  repeat
    :Get next "safe" book from queue;
    fork
      partition "External Tools" {
        :Execute `extract_chapter.py`\nto get bookmark JSON;
      }
    fork again
      partition "External Tools" {
        :Execute `pdftk dump_data`\nto get total page count;
      }
    end fork
    :Construct prompt with\nbookmark data and rules;
    partition "Gemini API" {
      :Send prompt via `get_gemini_response.py`\nand receive structured JSON;
    }
    if (Response is valid JSON?) then (yes)
      partition "External Tools" {
        :Execute `pdftk cat` commands;
      }
    else (no)
      partition "File System" {
        :Log failure with API error;
      }
    endif
  repeat while (More safe books in queue?) is (yes)
}

stop
@enduml
```


=========================================
FILE: segmentation_pipe.py
=========================================
```python
# segmentation_pipe.py (Enhanced with Quick Fixes)

#!/usr/bin/env python3
"""
Master orchestration script for the LLM-Powered PDF Segmentation Pipeline.
Enhanced with PyMuPDF slicing fallback, password cache, and partial success logging.
"""

import os
import json
import subprocess
import argparse
import sys
import pandas as pd
import re
from datetime import datetime, UTC
import fitz  # For fallback slicing

# --- System Components ---
from get_gemini_response import get_gemini_response
from prompt_generator import generate_segmentation_prompt
from extract_chapter import get_chapter_data, get_pdftk_metadata, remove_pdf_password_if_needed, get_page_count

# --- Configuration ---
OUTPUT_DIR = "segmented_output"
LOGS_DIR = "logs"
SEGMENTATION_LOG_FILE = os.path.join(LOGS_DIR, "segmentation.jsonl")
SKIPPED_LOG_FILE = os.path.join(LOGS_DIR, "skipped_books.txt")
CLASSIFICATIONS_FILE = "book_classifications.jsonl"

# Quick Fix 3: Password Cache
password_cache = {}  # {pdf_path: removed_status}

def get_cached_password_removal(pdf_path):
    if pdf_path not in password_cache:
        password_cache[pdf_path] = remove_pdf_password_if_needed(pdf_path)
    return password_cache[pdf_path]

def setup_directories():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(LOGS_DIR, exist_ok=True)

def load_processed_files() -> set:
    if not os.path.exists(SEGMENTATION_LOG_FILE):
        return set()
    processed = set()
    with open(SEGMENTATION_LOG_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line)
                if data.get('status') == 'SUCCESS':
                    processed.add(data['file_path'])
            except json.JSONDecodeError:
                continue
    return processed

def get_total_pages(pdf_path: str) -> int:
    """
    Wrapper for enhanced get_page_count with fallback.
    """
    # Quick Fix 3: Use cached password removal
    password_removed = get_cached_password_removal(pdf_path)
    if password_removed:
        print("  -> Password removed (cached); proceeding with page count.")
    return get_page_count(pdf_path)

def get_fallback_page_count(pdf_path: str) -> int:
    """
    PyMuPDF fallback for total pages if pdftk fully fails.
    """
    try:
        doc = fitz.open(pdf_path)
        return len(doc)
    except Exception as e:
        print(f"  -> PyMuPDF fallback failed for '{pdf_path}': {e}", file=sys.stderr)
        return 0

def log_result(file_path, status, message, commands_executed=0, commands_total=0):
    log_entry = {
        "timestamp": datetime.now(UTC).isoformat() + "Z",
        "file_path": file_path,
        "status": status,
        "message": message,
        "commands_executed": commands_executed,
        "commands_total": commands_total,
    }
    with open(SEGMENTATION_LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(json.dumps(log_entry) + '\n')

def sanitize_filename(name):
    """Removes invalid characters for file and directory names."""
    return re.sub(r'[<>:"/\\|?*\']', '_', name)

# Quick Fix 2: PyMuPDF Slicing Fallback Function
def extract_pages_pymupdf(pdf_path: str, start: int, end: int, output_filename: str) -> bool:
    """
    Fallback: Extract page range using PyMuPDF.
    start/end 1-indexed.
    """
    try:
        doc = fitz.open(pdf_path)
        output_doc = fitz.open()
        for p in range(start - 1, min(end, len(doc))):
            output_doc.insert_pdf(doc, from_page=p, to_page=p)
        output_doc.save(output_filename)
        output_doc.close()
        doc.close()
        return True
    except Exception as e:
        print(f"  -> PyMuPDF slicing error: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="Run the full PDF segmentation pipeline.")
    parser.add_argument('--force', action='store_true', help='Force reprocessing of all files.')
    args = parser.parse_args()

    setup_directories()
    
    print("--- Starting PDF Segmentation Pipeline ---")
    
    try:
        # --- ROBUST LOADING LOGIC ---
        records = []
        with open(CLASSIFICATIONS_FILE, 'r', encoding='utf-8') as f:
            full_content = f.read()
        corrected_content = full_content.replace('}{', '}\n{')
        for line in corrected_content.splitlines():
            if not line.strip():
                continue
            try:
                data = json.loads(line)
                record = {
                    'file_path': data.get('file_path'),
                    **data.get('classification_result', {}),
                    **data.get('final_evidence', {})
                }
                records.append(record)
            except json.JSONDecodeError as e:
                print(f"WARNING: Skipping malformed JSON in '{CLASSIFICATIONS_FILE}': {e}", file=sys.stderr)
        
        if not records:
            print(f"FATAL: No valid records in '{CLASSIFICATIONS_FILE}'.")
            sys.exit(1)

        df = pd.DataFrame(records)

    except Exception as e:
        print(f"FATAL: Could not parse '{CLASSIFICATIONS_FILE}': {e}", file=sys.stderr)
        sys.exit(1)

    processed_files = set() if args.force else load_processed_files()
    if not args.force:
        print(f"Found {len(processed_files)} previous successes. Skipping.")

    safe_mask = (df['analysis_type'] == 'metadata_check') & (~df['file_path'].isin(processed_files))
    safe_queue = df[safe_mask]
    unsafe_queue = df[~safe_mask]

    print(f"Found {len(safe_queue)} new books to process.")
    
    with open(SKIPPED_LOG_FILE, 'w', encoding='utf-8') as f:
        f.write(f"# Skipped Books Log - {datetime.now(UTC).isoformat()}\n")
        for _, row in unsafe_queue.iterrows():
            reason = "Already processed" if row['file_path'] in processed_files else f"analysis_type '{row['analysis_type']}'"
            f.write(f"{row['file_path']} | REASON: {reason}\n")
    print(f"Logged {len(unsafe_queue)} skips to '{SKIPPED_LOG_FILE}'.")

    for i, (_, book) in enumerate(safe_queue.iterrows()):
        pdf_path = book['file_path']
        print("\n" + "="*80)
        print(f"Processing ({i+1}/{len(safe_queue)}): {pdf_path}")
        print("="*80)
        
        # Quick Fix 3: Cached password removal
        password_removed = get_cached_password_removal(pdf_path)
        if password_removed:
            print("  -> Password removed (cached); retrying extraction.")
        
        print("  - Extracting bookmark data (PyPDF)...")
        try:
            bookmark_data = get_chapter_data(pdf_path)
        except Exception as e:
            print(f"  -> WARNING: PyPDF failed for '{pdf_path}': {e}", file=sys.stderr)
            bookmark_data = None
            
        print("  - Extracting pdftk metadata...")
        try:
            pdftk_metadata = get_pdftk_metadata(pdf_path)
        except Exception as e:
            print(f"  -> WARNING: pdftk failed for '{pdf_path}': {e}", file=sys.stderr)
            pdftk_metadata = None
        
        # Use PyPDF as primary; pdftk secondary
        if pdftk_metadata and bookmark_data:
            metadata_to_use = pdftk_metadata  # Prefer pdftk if both
        elif bookmark_data:
            print("  -> Using PyPDF-only metadata.")
            metadata_to_use = bookmark_data
            pdftk_metadata = None  # Flag for prompt
        elif pdftk_metadata:
            metadata_to_use = pdftk_metadata
        else:
            log_result(pdf_path, "FAILURE", "No metadata from PyPDF or pdftk.")
            continue
            
        print("  - Getting total page count...")
        total_pages = get_total_pages(pdf_path)
        if total_pages == 0:
            print("  -> pdftk page count failed. Falling back to PyMuPDF.")
            total_pages = get_fallback_page_count(pdf_path)
            if total_pages == 0:
                log_result(pdf_path, "FAILURE", "No page count from any source.")
                continue
            print(f"  -> Fallback page count: {total_pages}")
        
        # Proceed even with partial metadata
        if not metadata_to_use:
            log_result(pdf_path, "FAILURE", "No usable metadata after fallbacks.")
            continue
        
        try:
            # Adjust prompt for partial data
            prompt = generate_segmentation_prompt(
                bookmark_data if bookmark_data else metadata_to_use,
                pdftk_metadata,
                total_pages,
                pdf_path
            )
            raw_response = get_gemini_response(prompt, 'gemini-2.5-flash')
            cleaned_response = raw_response.replace("```json\n", "").replace("```", "")
            try:
                response_json = json.loads(cleaned_response)
            except Exception as e:
                print(f"  -> LLM parse error: {e}. Retrying with gemini-3-flash.")
                raw_response = get_gemini_response(prompt, 'gemini-3-flash')
                cleaned_response = raw_response.replace("```json\n", "").replace("```", "")
                response_json = json.loads(cleaned_response)
            
            if not response_json.get("segmentation_commands"):
                print("  -> Empty commands from LLM. Retrying with 2.5 pro model.")
                raw_response = get_gemini_response(prompt, 'gemini-2.5-pro')
                cleaned_response = raw_response.replace("```json\n", "").replace("```", "")
                response_json = json.loads(cleaned_response)
            
            if not response_json.get("segmentation_commands"):
                print("  -> Empty commands from LLM. Retrying with pro model.")
                raw_response = get_gemini_response(prompt, 'gemini-3-pro-preview')
                cleaned_response = raw_response.replace("```json\n", "").replace("```", "")
                response_json = json.loads(cleaned_response)
                
        except Exception as e:
            log_result(pdf_path, "FAILURE", f"LLM error: {e}")
            print(f"  -> ERROR: LLM failed: {e}", file=sys.stderr)
            continue

        commands = response_json.get("segmentation_commands", [])
        if not isinstance(commands, list):
            log_result(pdf_path, "FAILURE", "Invalid LLM structure.")
            print("  -> ERROR: Invalid LLM JSON.")
            continue
            
        if not commands:
            log_result(pdf_path, "SUCCESS", "LLM skipped due to insufficient metadata.")
            print("  -> INFO: LLM skipped safely.")
            continue

        commands_executed_count = 0
        try:
            base_name = sanitize_filename(os.path.splitext(os.path.basename(pdf_path))[0])
            # Handle relative path for output dir
            book_output_dir_parts = pdf_path.split(os.sep)[:-1]  # Use os.sep for cross-platform
            book_output_dir = os.path.join(OUTPUT_DIR, *book_output_dir_parts, base_name)
            os.makedirs(book_output_dir, exist_ok=True)
            print(f"  - Output directory: {book_output_dir}")

            for cmd_obj in commands:
                component_name = sanitize_filename(cmd_obj['component_name'])
                pdftk_command_template = cmd_obj['pdftk_command']
                output_filename = os.path.join(book_output_dir, f"{component_name}.pdf")
                final_command = pdftk_command_template.replace("IN_FILE", f'"{pdf_path}"').replace("OUT_FILE", f'"{output_filename}"')
                
                print(f"    - Executing: {final_command}")
                success = False
                # Quick Fix 2: pdftk with fallback
                try:
                    subprocess.run(final_command, shell=True, check=True, capture_output=True, timeout=120)
                    success = True
                except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:
                    print(f"    -> pdftk failed for {component_name}: {e}. Trying PyMuPDF fallback...")
                    # Parse page range from command (e.g., "cat 1-5" -> start=1, end=5)
                    page_match = re.search(r'cat\s+(\d+)-?(\d*)', final_command)
                    if page_match:
                        start = int(page_match.group(1))
                        end = int(page_match.group(2)) if page_match.group(2) else get_total_pages(pdf_path)
                        success = extract_pages_pymupdf(pdf_path, start, end, output_filename)
                        if success:
                            print(f"    -> PyMuPDF fallback succeeded for {component_name} (pages {start}-{end}).")
                        else:
                            print(f"    -> PyMuPDF fallback failed for {component_name}.")
                
                if success:
                    commands_executed_count += 1

            # Quick Fix 4: Accurate logging with threshold
            partial_success = (commands_executed_count / len(commands)) >= 0.5 if commands else False
            status = "SUCCESS" if partial_success else "PARTIAL_FAILURE"
            message = f"Segmentation partial ({commands_executed_count}/{len(commands)} extracted)."
            log_result(pdf_path, status, message, commands_executed_count, len(commands))
            if not partial_success:
                print("  -> PARTIAL FAILURE: <50% components extracted. Review log.")
            else:
                print("  -> SUCCESS: Book segmented.")

        except Exception as e:
            error_details = str(e)
            log_result(pdf_path, "FAILURE", f"Execution failed: {error_details}", commands_executed_count, len(commands))
            print(f"  -> ERROR: Execution failed: {error_details}", file=sys.stderr)
            continue

    print("\n--- Pipeline Finished ---")

if __name__ == "__main__":
    main()

```


=========================================
FILE: pipe.py
=========================================
```python
# pipe.py (Minor Enhancement: Add File Size Pre-Scan for Risky Files)

#!/usr/bin/env python3
"""
Master orchestration script for the Book Categorizer project.
Enhanced with file size logging for risky files.
"""

import os
import json
import argparse
import sys

# --- Import your custom modules ---
from metadata_checker import check_book_metadata
from layout_analyzer import analyze_book_layout
from ai_classifier import generate_classification_prompt
from get_gemini_response import get_gemini_response

# --- Configuration ---
SCAN_DIRECTORIES = ["BOOKS"]
OUTPUT_FILE = "book_classifications.jsonl"
FORCE_REPROCESS = False

def find_all_pdfs(directories: list) -> list:
    """Recursively finds all .pdf files in a list of directories."""
    pdf_files = []
    for directory in directories:
        for root, _, files in os.walk(directory):
            for file in files:
                if file.lower().endswith(".pdf"):
                    pdf_files.append(os.path.join(root, file))
    return sorted(pdf_files)

def load_processed_files(output_path: str) -> set:
    """Loads the set of already processed file paths from the output file."""
    if not os.path.exists(output_path):
        return set()
    
    processed = set()
    with open(output_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line)
                processed.add(data['final_evidence']['file'])
            except (json.JSONDecodeError, KeyError):
                continue
    return processed

def get_file_size_mb(pdf_path: str) -> float:
    """Get file size in MB for risk assessment."""
    try:
        return os.path.getsize(pdf_path) / (1024 * 1024)
    except:
        return 0

def process_single_book(pdf_path: str):
    """
    Runs the full analysis and classification pipeline for a single book.
    """
    size_mb = get_file_size_mb(pdf_path)
    if size_mb > 50:
        print(f"  -> WARNING: Large file ({size_mb:.1f} MB) - may timeout.")
    
    print("\n" + "="*80)
    print(f"Processing: {pdf_path} ({size_mb:.1f} MB)")
    print("="*80)

    # Stage 1: Metadata Check
    metadata_evidence = check_book_metadata(pdf_path)

    # This dictionary will hold all evidence sent to the AI
    final_evidence = metadata_evidence.copy()

    # Stage 2: Layout Analysis (if needed)
    if metadata_evidence.get("next_step") == "run_layout_analysis":
        layout_evidence = analyze_book_layout(pdf_path)
        if layout_evidence:
            # Merge layout evidence into the final evidence payload
            final_evidence.update(layout_evidence)
        else:
            print(f"-> WARNING: Layout analysis failed for {pdf_path}. Proceeding with metadata only.")

    # Stage 3: AI Classification
    print("\n--- Generating AI Prompt ---")
    prompt = generate_classification_prompt(final_evidence)
    
    ai_response = {
        "classification": "ERROR",
        "justification": "Failed to get response from AI.",
        "raw_output": ""
    }
    
    try:
        raw_response = get_gemini_response(prompt, "gemini-2.5-flash")
        print(f"RAW RESPONSE: {raw_response}")
        ai_response["raw_output"] = raw_response
        
        # Parse the structured response from the AI
        level_line = ""
        justification_line = ""
        for line in raw_response.splitlines():
            if line.upper().startswith("LEVEL:"):
                level_line = line.split(":", 1)[1].strip()
            elif line.upper().startswith("JUSTIFICATION:"):
                justification_line = line.split(":", 1)[1].strip()
        
        if level_line:
            ai_response["classification"] = level_line
            ai_response["justification"] = justification_line
        else:
            ai_response["justification"] = "AI response was not in the expected format."

    except Exception as e:
        print(f"-> ERROR: AI classification failed for {pdf_path}.", file=sys.stderr)
        print(f"   Details: {e}", file=sys.stderr)
        ai_response["justification"] = f"An exception occurred: {str(e)}"

    print("\n--- Classification Result ---")
    print(f"  Level: {ai_response['classification']}")
    print(f"  Justification: {ai_response['justification']}")

    # Combine all data into a single record
    final_record = {
        "file_path": pdf_path,
        "classification_result": ai_response,
        "final_evidence": final_evidence,
    }

    return final_record


def main():
    """
    Main function to orchestrate the entire classification process for the corpus.
    """
    parser = argparse.ArgumentParser(description="Run the full book classification pipeline.")
    parser.add_argument(
        '--force',
        action='store_true',
        help='Force reprocessing of all files, even if they exist in the output file.'
    )
    args = parser.parse_args()

    all_pdfs = find_all_pdfs(SCAN_DIRECTORIES)
    total_files = len(all_pdfs)
    print(f"Found {total_files} PDF files to process in {SCAN_DIRECTORIES}.")

    processed_files = set()
    if not args.force:
        processed_files = load_processed_files(OUTPUT_FILE)
        print(f"Found {len(processed_files)} already processed files. Will skip them.")
    else:
        print("Force reprocessing is enabled. All files will be processed.")
        # Clear the output file if forcing re-process
        if os.path.exists(OUTPUT_FILE):
            open(OUTPUT_FILE, 'w').close()


    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f_out:
        for i, pdf_path in enumerate(all_pdfs):
            
            if pdf_path in processed_files:
                print(f"({i+1}/{total_files}) Skipping already processed file: {pdf_path}")
                continue
                
            result_record = process_single_book(pdf_path)
            
            if result_record:
                # Write the result as a single line of JSON
                f_out.write(json.dumps(result_record) + '\n')
                f_out.flush() # Ensure data is written immediately

    print("\n" + "*"*80)
    print("Pipeline finished successfully.")
    print(f"All classification results have been saved to '{OUTPUT_FILE}'.")
    print("*"*80)


if __name__ == "__main__":
    main()

```


